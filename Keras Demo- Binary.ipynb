{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn a binary subset of the MNIST handwritten digit dataset. We will learn whether a digit is a zero or a one.\n",
    "\n",
    "#  Setting up the Data\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "\n",
    "Note: use Shift+Enter to run the codeblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib figure size and make inline\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 6]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into a training & testing set\n",
    "\n",
    "`x_train` & `x_test` are the examples, and `y_train` & `y_test` are the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples in full dataset: 60000\n",
      "Number of testing samples in full dataset: 10000\n",
      "\n",
      "Total number of binary examples with labels [0, 1] :  (14780, 785)\n",
      "Number of training samples in BINARY dataset: 10000\n",
      "Number of testing samples in BINARY dataset: 3780\n",
      "Number of validation samples in BINARY dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "# normalize the pixel values to be in the range (0, 1)\n",
    "x_train /= 255 \n",
    "x_test  /= 255\n",
    "\n",
    "\n",
    "print(\"Number of training samples in full dataset: \" + str(x_train.shape[0]))\n",
    "print(\"Number of testing samples in full dataset: \" + str(x_test.shape[0]))\n",
    "\n",
    "# Create a binary version for binary classification\n",
    "df = pd.DataFrame(np.vstack([x_train, x_test]))\n",
    "df['label'] = pd.Series(np.concatenate([y_train, y_test]))\n",
    "\n",
    "labels_to_separate = [0,1]\n",
    "\n",
    "binary_data = df[df['label'].isin(labels_to_separate)]\n",
    "\n",
    "# Create the binary dataset\n",
    "binary_x_train = binary_data.iloc[:10000].drop(['label'], axis=1).as_matrix()\n",
    "binary_y_train = binary_data.iloc[:10000]['label'].as_matrix()\n",
    "\n",
    "binary_x_validation = binary_data.iloc[10000:11000].drop(['label'], axis=1).as_matrix()\n",
    "binary_y_validation = binary_data.iloc[10000:11000]['label'].as_matrix()\n",
    "\n",
    "binary_x_test  = binary_data.iloc[11000:].drop(['label'], axis=1).as_matrix()\n",
    "binary_y_test  = binary_data.iloc[11000:]['label'].as_matrix()\n",
    "\n",
    "print()\n",
    "print(\"Total number of binary examples with labels\", str(labels_to_separate),\": \", binary_data.shape)\n",
    "print(\"Number of training samples in BINARY dataset: \" + str(binary_x_train.shape[0]))\n",
    "print(\"Number of testing samples in BINARY dataset: \" + str(binary_x_test.shape[0]))\n",
    "print(\"Number of validation samples in BINARY dataset: \" + str(binary_x_validation.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out what our data looks like\n",
    "\n",
    "Let's plot one of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of each image is(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183c4c3c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = binary_x_train[0]\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "image = np.reshape(image, [img_rows, img_cols])\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# The shape of each image is a vector with 784 binary values (\"pixels\")\n",
    "image_shape = binary_x_train[0].shape\n",
    "print(\"The shape of each image is\" + str(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make our model!!\n",
    "\n",
    "We're going to train a model that Keras calls a \"Sequential\" model.\n",
    "\n",
    "Sequential is an abstraction for really simple networks. There are many ways you can \"link\" the neurons between layers in a neural network, and \"Sequential\" is the simplest way to link these layers. Since we're training a single layer network, this doesn't really concern us.\n",
    "\n",
    "Sidenote: The alternative to in Keras to Sequential models are \"Functional\" models. You can use those to make fancier networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = Sequential()\n",
    "num_classes = 1\n",
    "binary_model.add(Dense(num_classes, activation='sigmoid', input_shape=image_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now that we've told the Keras *what the model is*, we now need to tell it *how to learn*.\n",
    "\n",
    "1. What's the loss function?\n",
    "\n",
    "  In class we've seen some loss functions like the hinge loss and the SVM loss. Here we're going to use a different loss function called the \"binary crossentropy\" loss. Cross-entropy is a loss function that works well for learning because it makes learning very fast when your function is \"very wrong\" but slower when it is pretty close to the true function. It is another name for the logistic loss that we briefly encountered in the previous lecture and will see a lot more of in coming lectures.\n",
    "\n",
    "2. What's the optimizer?\n",
    "\n",
    "  We'll use SGD (Stochastic Gradient Descent) which we've already discussed in class.\n",
    "\n",
    "3. Which metric to optimize?\n",
    "\n",
    "  We'll use accuracy- which is what we've been using for our algorithms all semester. There are some other options that make sense for other types of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.3047 - acc: 0.9449 - val_loss: 0.1488 - val_acc: 0.9960\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.1113 - acc: 0.9959 - val_loss: 0.0857 - val_acc: 0.9970\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.0733 - acc: 0.9963 - val_loss: 0.0623 - val_acc: 0.9970\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 0.0564 - acc: 0.9966 - val_loss: 0.0498 - val_acc: 0.9970\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 0.0467 - acc: 0.9970 - val_loss: 0.0421 - val_acc: 0.9970\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0403 - acc: 0.9970 - val_loss: 0.0368 - val_acc: 0.9970\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0358 - acc: 0.9970 - val_loss: 0.0330 - val_acc: 0.9970\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 0.0324 - acc: 0.9972 - val_loss: 0.0300 - val_acc: 0.9970\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0298 - acc: 0.9972 - val_loss: 0.0276 - val_acc: 0.9970\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0276 - acc: 0.9973 - val_loss: 0.0257 - val_acc: 0.9970\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = binary_model.fit(binary_x_train, binary_y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(binary_x_validation, binary_y_validation)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out that accuracies!\n",
    "\n",
    "99% accuracy!\n",
    "\n",
    "Let's try it on our *test set* instead of our *validation set* now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0221357902147\n",
      "Test accuracy: 0.998148148148\n"
     ]
    }
   ],
   "source": [
    "score = binary_model.evaluate(binary_x_test, binary_y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
